{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math as m\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib\n",
    "from fitter import Fitter\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "#from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.layers import Activation, Dense\n",
    "\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Repo\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  sentiment\n",
       "0  So there is no way for me to plug it in here i...          0\n",
       "1                        Good case, Excellent value.          1\n",
       "2                             Great for the jawbone.          1\n",
       "3  Tied to charger for conversations lasting more...          0\n",
       "4                                  The mic is great.          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset : Amazon Product Reviews\n",
    "data = pd.read_csv('//home//rahul//Downloads//sentiment labelled sentences//amazon_cells_labelled.txt', sep=\"\\t\", header = None)\n",
    "data.columns = [\"reviewText\", \"sentiment\"]\n",
    "print data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the SentimenText is not useful. It needs to be tokenized and cleaned.\n",
    "\n",
    "Here's my tokenizing function that splits each review into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "def tokenize(review_text):\n",
    "    try:\n",
    "        review_text = unicode(review_text.decode('utf-8').lower())\n",
    "        Ptokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = Ptokenizer.tokenize(review_text)\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the tokenization should now be cleaned to remove lines with 'NC', resulting from a tokenization error (usually due to weird encoding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1000/1000 [00:01<00:00, 799.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['reviewText'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now tokenized and cleaned. We are ready to feed it in the word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the word2vec model\n",
    "\n",
    "#### First, let's define a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(1000).tokens),\\\n",
    "                                                    np.array(data.head(1000).sentiment), test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([u'phone', u'works', u'great']),\n",
       "       list([u'wont', u'work', u'right', u'atleast']),\n",
       "       list([u'file', u'browser', u'offers', u'options', u'one', u'needs', u'handsfree', u'great']),\n",
       "       list([u'drain', u'weak', u'snap'])], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding lists of tokens into the word2vec model, we must turn them into LabeledSentence objects beforehand. Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [00:00, 246206.52it/s]\n",
      "300it [00:00, 88862.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeCallText(call_texts, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(call_texts)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeCallText(x_train, 'TRAIN')\n",
    "x_test = labelizeCallText(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first element from x_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=[u'none', u'tones', u'acceptable'], tags=['TRAIN_0'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so each element is basically some object with two attributes: a list (of tokens) and a label.\n",
    "Now we are ready to build the word2vec model from x_train i.e. the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 762402.70it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1272654.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3896"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_w2v = Word2Vec(size=200, min_count=10)\n",
    "review_text_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "review_text_w2v.train([x.words for x in tqdm(x_train)],total_examples=len(x_train), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is self-explanatory.\n",
    "* On the first line the model is initialized with the dimension of the vector space (we set it to 200) and min_count (a threshold for filtering words that appear less)\n",
    "* On the second line the vocabulary is created.\n",
    "* On the third line the model is trained i.e. its weights are updated.\n",
    "\n",
    "Once the model is built and trained on the corpus of call texts, we can use it to convert words to vectors. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.89024276e-04,   1.90061599e-03,   2.71252589e-04,\n",
       "        -5.41494228e-04,  -1.86419720e-03,   9.95092909e-04,\n",
       "        -1.61273812e-03,  -2.42068805e-03,   1.86959805e-03,\n",
       "        -1.79923096e-04,   1.39681890e-03,   2.29077390e-03,\n",
       "         1.17142117e-04,  -1.11752561e-04,  -2.25103134e-03,\n",
       "        -8.53066391e-04,  -9.32306226e-04,   1.45286089e-03,\n",
       "         4.81377152e-04,   1.78067293e-03,   1.51930714e-03,\n",
       "        -2.27694702e-03,   2.47723539e-03,   1.33213797e-03,\n",
       "        -2.21236981e-03,   2.24324293e-03,   9.10254585e-06,\n",
       "        -7.18741561e-04,  -1.87078875e-03,  -4.00763856e-05,\n",
       "        -1.05708803e-03,  -2.02402729e-03,   9.44534724e-04,\n",
       "         1.32431998e-03,   1.84042682e-03,   5.91372489e-04,\n",
       "         9.42600949e-04,  -1.74512935e-03,  -1.32808601e-03,\n",
       "        -1.38255360e-03,   1.10342167e-04,  -2.11775349e-03,\n",
       "        -1.81310414e-03,   1.66405435e-03,   1.68973929e-03,\n",
       "        -2.26604869e-03,   7.97806249e-04,   2.42115231e-03,\n",
       "        -1.13030977e-03,  -1.30949030e-03,   6.69517263e-04,\n",
       "         1.51058182e-03,  -1.50541076e-04,   2.61320919e-03,\n",
       "         1.40663108e-03,   9.56630101e-04,   2.82579742e-04,\n",
       "        -7.19671196e-04,   7.44993915e-04,   1.24038692e-04,\n",
       "        -5.08791592e-04,  -2.47509684e-04,  -3.83958162e-04,\n",
       "        -2.54879007e-03,   1.15257816e-03,   1.53620538e-04,\n",
       "         2.16963026e-03,  -7.27869978e-04,   9.84080951e-04,\n",
       "        -2.53018877e-03,   9.28052119e-04,  -2.02290481e-03,\n",
       "         1.06036570e-03,   2.11288643e-04,  -2.40838272e-03,\n",
       "        -3.70490219e-04,  -7.88026548e-04,   1.13495311e-03,\n",
       "         2.03233934e-03,   2.42472743e-03,  -1.90028176e-03,\n",
       "        -1.25521922e-03,  -4.68796119e-04,  -2.32763053e-03,\n",
       "        -1.82268419e-03,   5.77350380e-04,  -5.29788551e-04,\n",
       "        -1.45182572e-03,   1.91574579e-03,   2.38058413e-03,\n",
       "        -1.68633240e-03,   6.65102561e-04,   8.05579184e-04,\n",
       "         7.15359856e-05,   1.20032823e-03,  -2.34394614e-03,\n",
       "         6.12607342e-04,   2.47881492e-03,   7.40527757e-04,\n",
       "         6.21212414e-04,   1.39960530e-03,  -1.76917273e-03,\n",
       "        -2.33258423e-03,   8.26778262e-07,   1.19260256e-03,\n",
       "        -1.99173670e-03,  -1.53159851e-03,   5.43755887e-04,\n",
       "        -1.74032873e-03,   4.82506526e-04,   1.27147371e-03,\n",
       "         1.93900359e-03,   1.72585610e-03,  -1.74787396e-03,\n",
       "        -1.65733672e-03,   2.14754906e-03,  -1.73249503e-03,\n",
       "        -2.99632869e-04,  -2.22639908e-04,   2.98695406e-04,\n",
       "         9.91666922e-04,   1.77222060e-03,  -4.42515215e-04,\n",
       "        -1.93596689e-03,  -2.03898642e-04,  -3.19295141e-05,\n",
       "         7.08905107e-04,  -1.59217720e-03,  -1.64988905e-03,\n",
       "         8.17210472e-04,   1.39742333e-03,   1.54398521e-03,\n",
       "         9.78511642e-04,  -2.11068220e-03,   9.84125305e-04,\n",
       "        -1.64804049e-03,  -2.41851667e-03,  -1.93296804e-03,\n",
       "         9.34348442e-04,  -2.48029758e-03,  -7.74109561e-04,\n",
       "         2.25030095e-03,   2.34792288e-03,  -9.76404699e-04,\n",
       "         1.79768214e-03,   7.78463844e-04,   1.54739292e-03,\n",
       "        -1.94861332e-05,   5.59252687e-04,   1.38481113e-03,\n",
       "        -1.21577794e-03,   1.79370749e-03,  -2.01112777e-03,\n",
       "        -1.72361557e-03,  -1.13015319e-03,   8.30679724e-04,\n",
       "         1.83155655e-03,  -2.11591530e-03,  -1.07090781e-03,\n",
       "        -2.56447797e-03,  -1.95087679e-03,   2.29925057e-03,\n",
       "        -1.94541959e-03,   8.95604433e-04,  -9.43214283e-04,\n",
       "        -2.43689003e-03,   1.43216096e-03,   1.49897789e-03,\n",
       "         5.14516607e-04,  -1.89866812e-03,  -2.47877510e-03,\n",
       "         1.56495883e-03,  -2.42558727e-03,   1.38799869e-03,\n",
       "         1.60094583e-03,  -8.84547830e-04,  -4.83667711e-04,\n",
       "         8.64182366e-04,  -2.04094290e-03,  -8.29929078e-04,\n",
       "         3.40350845e-04,  -1.69840117e-03,   2.05251412e-03,\n",
       "        -7.44628254e-04,   1.71784894e-03,   5.14060492e-04,\n",
       "         7.84626696e-04,  -9.57178127e-04,  -2.47825606e-04,\n",
       "        -2.48612557e-03,   1.64318702e-03,  -7.98971567e-04,\n",
       "        -1.53182296e-03,   1.17968698e-03,   2.17079045e-03,\n",
       "         1.14693411e-03,   1.54937175e-03,   5.61008172e-04,\n",
       "         2.16180691e-03,  -1.13405089e-03], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_w2v['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check: this is a 200-dimension vector. Of course, we can only get the vectors of the words of the corpus.\n",
    "Let's try something else. We spoke earlier about semantic relationships. Well, the Word2Vec gensim implementation provides a cool method named most_similar. Given a word, this method returns the top n similar ones. This is an interesting feature. Let's try it on some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'also', 0.2034391462802887),\n",
       " (u'one', 0.17719045281410217),\n",
       " (u'time', 0.16582369804382324),\n",
       " (u'work', 0.15659163892269135),\n",
       " (u'great', 0.11894215643405914),\n",
       " (u'excellent', 0.1180928573012352),\n",
       " (u'fit', 0.11151019483804703),\n",
       " (u'2', 0.11087364703416824),\n",
       " (u'really', 0.10830893367528915),\n",
       " (u'ever', 0.10527921468019485)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How awesome is that?\n",
    "\n",
    "For a given word, we get similar surrounding words of same context. Basically these words have a probability to be closer to that given word in most of the call texts.\n",
    "\n",
    "It's interesting to see that our model gets facebook, twitter, skype together and bar, restaurant and cafe together as well. This could be useful for building a knowledge graph. Any thoughts about that?\n",
    "\n",
    "How about visualizing these word vectors? We first have to reduce their dimension to 2 using t-SNE. Then, using an interactive visualization tool such as Bokeh, we can map them directly on 2D plane and interact with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get to the sentiment classification part. As for now, we have a word2vec model that converts each word from the corpus into a high dimensional vector. This seems to work fine according to the similarity tests.\n",
    "\n",
    "In order to classify call texts, we have to turn them into vectors as well. How could we do this? Well, this task is almost done. Since we know the vector representation of each word composing a call texts, we have to \"combine\" these vectors together and get a new one that represents the reviews as a whole.\n",
    "\n",
    "A first approach consists in averaging the word vectors together. But a slightly better solution I found was to compute a weighted average where each weight gives the importance of the word with respect to the corpus. Such a weight could the tf-idf score. To learn more about tf-idf, you can look at <> article.\n",
    "\n",
    "Let's start by building a tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 62\n"
     ]
    }
   ],
   "source": [
    "print 'building tf-idf matrix ...'\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print 'vocab size :', len(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function that, given a list of call text tokens, creates an averaged review vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += review_text_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert x_train and and x_test into list of vectors using this function. We also scale each column to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 26409.17it/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 26636.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be ready to feed these vectors into a neural network classifier. In fact, using Keras is very easy to define layers and activation functions.\n",
    "Here is a basic 2-layer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "0s - loss: 0.7215 - acc: 0.5900\n",
      "Epoch 2/10\n",
      "0s - loss: 0.5685 - acc: 0.7186\n",
      "Epoch 3/10\n",
      "0s - loss: 0.5230 - acc: 0.7414\n",
      "Epoch 4/10\n",
      "0s - loss: 0.4975 - acc: 0.7700\n",
      "Epoch 5/10\n",
      "0s - loss: 0.4774 - acc: 0.7829\n",
      "Epoch 6/10\n",
      "0s - loss: 0.4641 - acc: 0.7900\n",
      "Epoch 7/10\n",
      "0s - loss: 0.4536 - acc: 0.7986\n",
      "Epoch 8/10\n",
      "0s - loss: 0.4479 - acc: 0.7886\n",
      "Epoch 9/10\n",
      "0s - loss: 0.4327 - acc: 0.8014\n",
      "Epoch 10/10\n",
      "0s - loss: 0.4283 - acc: 0.7943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce366b5a50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train,  batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.720000003179\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.52593540668487548, 0.72000000317891444]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72% accuracy in the first swing of the model is not bad. We could eventually tune more parameters in the word2vec model and the neural network classifer to reach a higher precision score. \n",
    "\n",
    "Please tell me if you managed to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
